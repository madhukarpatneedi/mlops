{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using custom containers with AI Platform Training\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Learn how to create a train and a validation split with Big Query\n",
    "1. Learn how to wrap a machine learning model into a Docker container and train in on CAIP\n",
    "1. Learn how to use the hyperparameter tunning engine on GCP to find the best hyperparameters\n",
    "1. Learn how to deploy a trained machine learning model GCP as a rest API and query it\n",
    "\n",
    "In this lab, you develop, package as a docker image, and run on **AI Platform Training** a training application that trains a multi-class classification model that predicts the type of forest cover from cartographic data. The [dataset](../../../datasets/covertype/README.md) used in the lab is based on **Covertype Data Set** from UCI Machine Learning Repository.\n",
    "\n",
    "The training code uses `scikit-learn` for data pre-processing and modeling. The code has been instrumented using the `hypertune` package so it can be used with **AI Platform** hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set location paths, connections strings, and other environment settings. Make sure to update   `REGION`, and `ARTIFACT_STORE`  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for AI Platform Training and Prediction\n",
    "- `ARTIFACT_STORE` - the GCS bucket created during installation of AI Platform Pipelines. The bucket name starts with the `hostedkfp-default-` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.dmgcp-pkg-internal-poc-oct-04.appspot.com/\n",
      "gs://datametica_internal/\n",
      "gs://delete_keras_demo_wit/\n",
      "gs://delete_mortgage_xgboost/\n",
      "gs://dm_internal_demo/\n",
      "gs://dmgcp-pkg-internal-poc-oct-04/\n",
      "gs://dmgcp-pkg-internal-poc-oct-04-kubeflowpipelines-default/\n",
      "gs://dmgcp-pkg-internal-poc-oct-04-mlengine/\n",
      "gs://dmgcp-pkg-internal-poc-oct-04.appspot.com/\n",
      "gs://dmgcp-pkg-internal-poc-oct-04_cloudbuild/\n",
      "gs://europe-west1-my-composer-c04144a9-bucket/\n",
      "gs://fastai-demo/\n",
      "gs://hostedkfp-default-t0ns1asff6/\n",
      "gs://image_classification_kyc/\n",
      "gs://kyc_processing/\n",
      "gs://qvc_femo_files/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://hostedkfp-default-t0ns1asff6'\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'dataset.csv')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://hostedkfp-default-t0ns1asff6/data/validation/dataset.csv'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Credit card dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7444.000000000</td>\n",
       "      <td>-1.732699558</td>\n",
       "      <td>-0.226281615</td>\n",
       "      <td>1.373927393</td>\n",
       "      <td>0.868954431</td>\n",
       "      <td>3.696451906</td>\n",
       "      <td>3.827377978</td>\n",
       "      <td>-0.897345356</td>\n",
       "      <td>0.537505032</td>\n",
       "      <td>1.490803114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.410690459</td>\n",
       "      <td>-0.366613948</td>\n",
       "      <td>-0.667907407</td>\n",
       "      <td>0.982553936</td>\n",
       "      <td>-0.317467795</td>\n",
       "      <td>-0.333377955</td>\n",
       "      <td>-0.498264124</td>\n",
       "      <td>-0.310378381</td>\n",
       "      <td>0E-9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8256.000000000</td>\n",
       "      <td>0.919153734</td>\n",
       "      <td>0.148087303</td>\n",
       "      <td>1.681714330</td>\n",
       "      <td>2.779928578</td>\n",
       "      <td>-0.105434428</td>\n",
       "      <td>2.032115669</td>\n",
       "      <td>-1.000826622</td>\n",
       "      <td>0.679995612</td>\n",
       "      <td>1.405340838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099649840</td>\n",
       "      <td>0.249609434</td>\n",
       "      <td>0.228987663</td>\n",
       "      <td>-0.717434874</td>\n",
       "      <td>-0.095069397</td>\n",
       "      <td>0.027851205</td>\n",
       "      <td>0.086004683</td>\n",
       "      <td>0.017047698</td>\n",
       "      <td>0E-9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26373.000000000</td>\n",
       "      <td>-1.563484871</td>\n",
       "      <td>1.155879702</td>\n",
       "      <td>1.924913297</td>\n",
       "      <td>1.512296159</td>\n",
       "      <td>-0.447353620</td>\n",
       "      <td>1.280312133</td>\n",
       "      <td>-0.888981010</td>\n",
       "      <td>1.433368488</td>\n",
       "      <td>-0.470963337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170774257</td>\n",
       "      <td>0.454554592</td>\n",
       "      <td>-0.096082711</td>\n",
       "      <td>-0.276239147</td>\n",
       "      <td>-0.356016680</td>\n",
       "      <td>-0.210269702</td>\n",
       "      <td>-0.172889768</td>\n",
       "      <td>-0.015972867</td>\n",
       "      <td>0E-9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35377.000000000</td>\n",
       "      <td>1.126534327</td>\n",
       "      <td>0.289679104</td>\n",
       "      <td>1.537309696</td>\n",
       "      <td>2.706547133</td>\n",
       "      <td>-0.759367905</td>\n",
       "      <td>0.161174018</td>\n",
       "      <td>-0.529164795</td>\n",
       "      <td>0.141161868</td>\n",
       "      <td>0.046730494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081740974</td>\n",
       "      <td>-0.059833686</td>\n",
       "      <td>0.041016588</td>\n",
       "      <td>0.398585954</td>\n",
       "      <td>0.312066335</td>\n",
       "      <td>-0.045035054</td>\n",
       "      <td>0.045694022</td>\n",
       "      <td>0.036644637</td>\n",
       "      <td>0E-9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>147501.000000000</td>\n",
       "      <td>-1.611877338</td>\n",
       "      <td>-0.408409923</td>\n",
       "      <td>-3.829761676</td>\n",
       "      <td>6.249461761</td>\n",
       "      <td>-3.360921609</td>\n",
       "      <td>1.147963582</td>\n",
       "      <td>1.858424853</td>\n",
       "      <td>0.474858318</td>\n",
       "      <td>-3.838398816</td>\n",
       "      <td>...</td>\n",
       "      <td>1.245582186</td>\n",
       "      <td>0.616382702</td>\n",
       "      <td>2.251438880</td>\n",
       "      <td>-0.066096392</td>\n",
       "      <td>0.538710327</td>\n",
       "      <td>0.541325493</td>\n",
       "      <td>-0.136243382</td>\n",
       "      <td>-0.009852256</td>\n",
       "      <td>996.270000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>139951.000000000</td>\n",
       "      <td>-2.921944376</td>\n",
       "      <td>-0.228062381</td>\n",
       "      <td>-5.877288646</td>\n",
       "      <td>2.201883522</td>\n",
       "      <td>-1.935439873</td>\n",
       "      <td>0.631140872</td>\n",
       "      <td>-1.245106342</td>\n",
       "      <td>1.511348020</td>\n",
       "      <td>-1.899986657</td>\n",
       "      <td>...</td>\n",
       "      <td>1.441622275</td>\n",
       "      <td>0.895527949</td>\n",
       "      <td>1.385511128</td>\n",
       "      <td>-2.028024229</td>\n",
       "      <td>0.509130972</td>\n",
       "      <td>0.172642921</td>\n",
       "      <td>0.726781012</td>\n",
       "      <td>0.234513922</td>\n",
       "      <td>723.210000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>131024.000000000</td>\n",
       "      <td>0.469749517</td>\n",
       "      <td>-1.237555470</td>\n",
       "      <td>-1.767340616</td>\n",
       "      <td>4.833490128</td>\n",
       "      <td>-0.268714559</td>\n",
       "      <td>-0.512759848</td>\n",
       "      <td>1.140148806</td>\n",
       "      <td>-0.341273000</td>\n",
       "      <td>-1.046350984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303905313</td>\n",
       "      <td>-0.647075252</td>\n",
       "      <td>-0.373013646</td>\n",
       "      <td>0.260801138</td>\n",
       "      <td>-0.496566412</td>\n",
       "      <td>-0.245972762</td>\n",
       "      <td>-0.117857669</td>\n",
       "      <td>0.144774181</td>\n",
       "      <td>723.210000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>122608.000000000</td>\n",
       "      <td>-2.003459531</td>\n",
       "      <td>-7.159041717</td>\n",
       "      <td>-4.050976316</td>\n",
       "      <td>1.309579747</td>\n",
       "      <td>-2.058101588</td>\n",
       "      <td>-0.098620927</td>\n",
       "      <td>2.880082727</td>\n",
       "      <td>-0.727484047</td>\n",
       "      <td>1.460380551</td>\n",
       "      <td>...</td>\n",
       "      <td>1.244286775</td>\n",
       "      <td>-1.015232287</td>\n",
       "      <td>-1.800984866</td>\n",
       "      <td>0.657585627</td>\n",
       "      <td>-0.435617247</td>\n",
       "      <td>-0.894508922</td>\n",
       "      <td>-0.397557387</td>\n",
       "      <td>0.314261714</td>\n",
       "      <td>2125.870000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284807</th>\n",
       "      <td>70536.000000000</td>\n",
       "      <td>-2.271754537</td>\n",
       "      <td>-0.457654561</td>\n",
       "      <td>-2.589054639</td>\n",
       "      <td>2.230778267</td>\n",
       "      <td>-4.278982503</td>\n",
       "      <td>0.388610080</td>\n",
       "      <td>0.102485457</td>\n",
       "      <td>0.813127838</td>\n",
       "      <td>-1.092920839</td>\n",
       "      <td>...</td>\n",
       "      <td>1.096342010</td>\n",
       "      <td>0.658398655</td>\n",
       "      <td>1.711675735</td>\n",
       "      <td>0.333540036</td>\n",
       "      <td>0.538591374</td>\n",
       "      <td>-0.193528512</td>\n",
       "      <td>0.258194101</td>\n",
       "      <td>0.247269214</td>\n",
       "      <td>824.830000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284808 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Time            V1            V2            V3  \\\n",
       "0                   None          None          None          None   \n",
       "1         7444.000000000  -1.732699558  -0.226281615   1.373927393   \n",
       "2         8256.000000000   0.919153734   0.148087303   1.681714330   \n",
       "3        26373.000000000  -1.563484871   1.155879702   1.924913297   \n",
       "4        35377.000000000   1.126534327   0.289679104   1.537309696   \n",
       "...                  ...           ...           ...           ...   \n",
       "284803  147501.000000000  -1.611877338  -0.408409923  -3.829761676   \n",
       "284804  139951.000000000  -2.921944376  -0.228062381  -5.877288646   \n",
       "284805  131024.000000000   0.469749517  -1.237555470  -1.767340616   \n",
       "284806  122608.000000000  -2.003459531  -7.159041717  -4.050976316   \n",
       "284807   70536.000000000  -2.271754537  -0.457654561  -2.589054639   \n",
       "\n",
       "                 V4            V5            V6            V7            V8  \\\n",
       "0              None          None          None          None          None   \n",
       "1       0.868954431   3.696451906   3.827377978  -0.897345356   0.537505032   \n",
       "2       2.779928578  -0.105434428   2.032115669  -1.000826622   0.679995612   \n",
       "3       1.512296159  -0.447353620   1.280312133  -0.888981010   1.433368488   \n",
       "4       2.706547133  -0.759367905   0.161174018  -0.529164795   0.141161868   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "284803  6.249461761  -3.360921609   1.147963582   1.858424853   0.474858318   \n",
       "284804  2.201883522  -1.935439873   0.631140872  -1.245106342   1.511348020   \n",
       "284805  4.833490128  -0.268714559  -0.512759848   1.140148806  -0.341273000   \n",
       "284806  1.309579747  -2.058101588  -0.098620927   2.880082727  -0.727484047   \n",
       "284807  2.230778267  -4.278982503   0.388610080   0.102485457   0.813127838   \n",
       "\n",
       "                  V9  ...           V21           V22           V23  \\\n",
       "0               None  ...          None          None          None   \n",
       "1        1.490803114  ...  -0.410690459  -0.366613948  -0.667907407   \n",
       "2        1.405340838  ...  -0.099649840   0.249609434   0.228987663   \n",
       "3       -0.470963337  ...   0.170774257   0.454554592  -0.096082711   \n",
       "4        0.046730494  ...  -0.081740974  -0.059833686   0.041016588   \n",
       "...              ...  ...           ...           ...           ...   \n",
       "284803  -3.838398816  ...   1.245582186   0.616382702   2.251438880   \n",
       "284804  -1.899986657  ...   1.441622275   0.895527949   1.385511128   \n",
       "284805  -1.046350984  ...   0.303905313  -0.647075252  -0.373013646   \n",
       "284806   1.460380551  ...   1.244286775  -1.015232287  -1.800984866   \n",
       "284807  -1.092920839  ...   1.096342010   0.658398655   1.711675735   \n",
       "\n",
       "                 V24           V25           V26           V27           V28  \\\n",
       "0               None          None          None          None          None   \n",
       "1        0.982553936  -0.317467795  -0.333377955  -0.498264124  -0.310378381   \n",
       "2       -0.717434874  -0.095069397   0.027851205   0.086004683   0.017047698   \n",
       "3       -0.276239147  -0.356016680  -0.210269702  -0.172889768  -0.015972867   \n",
       "4        0.398585954   0.312066335  -0.045035054   0.045694022   0.036644637   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "284803  -0.066096392   0.538710327   0.541325493  -0.136243382  -0.009852256   \n",
       "284804  -2.028024229   0.509130972   0.172642921   0.726781012   0.234513922   \n",
       "284805   0.260801138  -0.496566412  -0.245972762  -0.117857669   0.144774181   \n",
       "284806   0.657585627  -0.435617247  -0.894508922  -0.397557387   0.314261714   \n",
       "284807   0.333540036   0.538591374  -0.193528512   0.258194101   0.247269214   \n",
       "\n",
       "                Amount Class  \n",
       "0                 None   NaN  \n",
       "1                 0E-9   0.0  \n",
       "2                 0E-9   0.0  \n",
       "3                 0E-9   0.0  \n",
       "4                 0E-9   0.0  \n",
       "...                ...   ...  \n",
       "284803   996.270000000   1.0  \n",
       "284804   723.210000000   1.0  \n",
       "284805   723.210000000   1.0  \n",
       "284806  2125.870000000   1.0  \n",
       "284807   824.830000000   1.0  \n",
       "\n",
       "[284808 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM `MLOps_poc.credit_card_fraud`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation splits\n",
    "\n",
    "Use BigQuery to sample training and validation splits and save them to GCS storage\n",
    "### Create a training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r593b9a6198bde704_000001740eed79eb_1 ... (10s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table MLOps_poc.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `MLOps_poc.credit_card_fraud` AS credit \\\n",
    "WHERE \\\n",
    "rand() < 0.8' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r71f6c45f60046c86_000001740eedaf1b_1 ... (10s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "MLOps_poc.training \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r3829f7fc73fdb85b_000001740eeaf4da_1 ... (10s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table MLOps_poc.training1 \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `MLOps_poc.credit_card_fraud` AS credit \\\n",
    "WHERE \\\n",
    "rand() < 0.7' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r6659df1b4040b655_000001740eeb32dd_1 ... (6s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "MLOps_poc.training1 \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r63a117c14d5604a_000001740eede488_1 ... (4s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table MLOps_poc.validation \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `MLOps_poc.credit_card_fraud` AS credit \\\n",
    "WHERE \\\n",
    "rand() < 0.2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r54bb20a8313a8acb_000001740eee037d_1 ... (3s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "MLOps_poc.validation \\\n",
    "$VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227723, 31)\n",
      "(57256, 31)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a training application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the `sklearn` training pipeline.\n",
    "\n",
    "The training pipeline preprocesses data by standardizing all numeric features using `sklearn.preprocessing.StandardScaler` and encoding all categorical features using `sklearn.preprocessing.OneHotEncoder`. It uses stochastic gradient descent linear classifier (`SGDClassifier`) for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "numeric_feature_indexes = slice(0, 30)\n",
    "categorical_feature_indexes = slice(31)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes), \n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(max_depth=3))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all numeric features to `float64`\n",
    "\n",
    "To avoid warning messages from `StandardScaler` all numeric features are converted to `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "\n",
    "df_train = df_train.astype(num_features_type_map)\n",
    "df_validation = df_validation.astype(num_features_type_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                  slice(0, 30, None))])),\n",
       "                ('classifier', RandomForestClassifier(max_depth=3))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train.drop('Class', axis=1)\n",
    "y_train = df_train['Class']\n",
    "X_validation = df_validation.drop('Class', axis=1)\n",
    "y_validation = df_validation['Class']\n",
    "\n",
    "pipeline.set_params(classifier__max_depth=3)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the trained model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9991616599133715\n"
     ]
    }
   ],
   "source": [
    "accuracy = pipeline.score(X_validation, y_validation)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the hyperparameter tuning application.\n",
    "Since the training run on this dataset is computationally expensive you can benefit from running a distributed hyperparameter tuning job on AI Platform Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the tuning script. \n",
    "\n",
    "Notice the use of the `hypertune` package to report the `accuracy` optimization metric to AI Platform hyperparameter tuning service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_feature_indexes = slice(0, 30)\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes)\n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier',RandomForestClassifier(max_depth=2))\n",
    "    ])\n",
    "\n",
    "    num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Class', axis=1)\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    pipeline.set_params(classifier__max_depth=alpha)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        X_validation = df_validation.drop('Class', axis=1)\n",
    "        y_validation = df_validation['Class']\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6 | packaged by conda-forge | (default, Jun  1 2020, 18:57:50) \n",
      "[GCC 7.5.0] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> \n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "KeyboardInterrupt\n",
      ">>> "
     ]
    }
   ],
   "source": [
    "!python  helper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "Notice that we are installing specific versions of `scikit-learn` and `pandas` in the training image. This is done to make sure that the training runtime is aligned with the serving runtime. Later in the notebook you will deploy the model to AI Platform Prediction, using the 1.15 version of AI Platform Prediction runtime. \n",
    "\n",
    "Make sure to update the URI for the base image so that it points to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the docker image. \n",
    "\n",
    "You use **Cloud Build** to build the image and push it your project's **Container Registry**. As you use the remote cloud service to build the image, you don't need a local installation of Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 3.1 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://dmgcp-pkg-internal-poc-oct-04_cloudbuild/source/1597969559.67-a53216f0e0ad4001a65f6888a6b54051.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/dmgcp-pkg-internal-poc-oct-04/builds/226d8e4e-0620-4a17-bfb9-22035dee1bbf].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/226d8e4e-0620-4a17-bfb9-22035dee1bbf?project=57394649199].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"226d8e4e-0620-4a17-bfb9-22035dee1bbf\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://dmgcp-pkg-internal-poc-oct-04_cloudbuild/source/1597969559.67-a53216f0e0ad4001a65f6888a6b54051.tgz#1597969560142365\n",
      "Copying gs://dmgcp-pkg-internal-poc-oct-04_cloudbuild/source/1597969559.67-a53216f0e0ad4001a65f6888a6b54051.tgz#1597969560142365...\n",
      "/ [1 files][  1.5 KiB/  1.5 KiB]                                                \n",
      "Operation completed over 1 objects/1.5 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  5.632kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 2286ed3e6644\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=60baa32a34c6bd98483d81bcf50b394f83942ce1b8b45de3ed2aca4d5adc08ca\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=2824b65d0f072d93128912d081d260888831d7af57377725581729e3944dc5f2\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=65eb8f4426311e3625ff1ead08a7c5176cb8f522a29b2349e40e1aa0e8dbdf14\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 2286ed3e6644\n",
      " ---> 53e01f8da2d1\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 0abd8492c4e9\n",
      "Removing intermediate container 0abd8492c4e9\n",
      " ---> a8618e3c79f8\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 759411c633c0\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 79d5c2bc2c8d\n",
      "Removing intermediate container 79d5c2bc2c8d\n",
      " ---> 05619e98a023\n",
      "Successfully built 05619e98a023\n",
      "Successfully tagged gcr.io/dmgcp-pkg-internal-poc-oct-04/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/dmgcp-pkg-internal-poc-oct-04/trainer_image:latest\n",
      "The push refers to repository [gcr.io/dmgcp-pkg-internal-poc-oct-04/trainer_image]\n",
      "d37663018f71: Preparing\n",
      "f98be25a4041: Preparing\n",
      "4ec1134a678c: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "89212ed9ad75: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "d37663018f71: Pushed\n",
      "f98be25a4041: Pushed\n",
      "5133f6c43556: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "4ec1134a678c: Pushed\n",
      "latest: digest: sha256:49e4f35267b6eaa691eae46bd6712c4812df26921bdeac264a4a40d078bd0431 size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                   IMAGES                                                        STATUS\n",
      "226d8e4e-0620-4a17-bfb9-22035dee1bbf  2020-08-21T00:26:00+00:00  3M30S     gs://dmgcp-pkg-internal-poc-oct-04_cloudbuild/source/1597969559.67-a53216f0e0ad4001a65f6888a6b54051.tgz  gcr.io/dmgcp-pkg-internal-poc-oct-04/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit an AI Platform hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the hyperparameter configuration file. \n",
    "Recall that the training code uses `SGDClassifier`. The training application has been designed to accept two hyperparameters that control `SGDClassifier`:\n",
    "- Max iterations\n",
    "- Alpha\n",
    "\n",
    "The below file configures AI Platform hypertuning to run up to 6 trials on up to three nodes and to choose from two discrete values of `max_iter` and the linear range betwee 0.00001 and 0.001 for `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_iter\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          200,\n",
    "          500\n",
    "          ]\n",
    "    - parameterName: alpha\n",
    "      type: DISCRETE\n",
    "      minValue:  2\n",
    "      maxValue:  4\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the hyperparameter tuning job.\n",
    "\n",
    "Use the `gcloud` command to start the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200821_003637] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200821_003637\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200821_003637\n",
      "jobId: JOB_20200821_003637\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path=$VALIDATION_FILE_PATH \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the job.\n",
    "\n",
    "You can monitor the job using GCP console or from within the notebook using `gcloud` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2020-08-21T00:36:39Z'\n",
      "etag: jHjWWc5djVk=\n",
      "jobId: JOB_20200821_003637\n",
      "startTime: '2020-08-21T00:36:41Z'\n",
      "state: RUNNING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset_path=gs://hostedkfp-default-t0ns1asff6/data/training/dataset.csv\n",
      "  - --validation_dataset_path=gs://hostedkfp-default-t0ns1asff6/data/validation/dataset.csv\n",
      "  - --hptune\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 4\n",
      "    maxTrials: 4\n",
      "    params:\n",
      "    - discreteValues:\n",
      "      - 200.0\n",
      "      - 500.0\n",
      "      parameterName: max_iter\n",
      "      type: DISCRETE\n",
      "    - maxValue: 4.0\n",
      "      minValue: 2.0\n",
      "      parameterName: alpha\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: DOUBLE\n",
      "  jobDir: gs://hostedkfp-default-t0ns1asff6/jobs/JOB_20200821_003637\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/dmgcp-pkg-internal-poc-oct-04/trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  hyperparameterMetricTag: accuracy\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20200821_003637?project=dmgcp-pkg-internal-poc-oct-04\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2FJOB_20200821_003637&project=dmgcp-pkg-internal-poc-oct-04\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job completes you can review the results using GCP Console or programatically by calling the AI Platform Training REST POI-point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20200821_003637',\n",
       " 'trainingInput': {'args': ['--training_dataset_path=gs://hostedkfp-default-t0ns1asff6/data/training/dataset.csv',\n",
       "   '--validation_dataset_path=gs://hostedkfp-default-t0ns1asff6/data/validation/dataset.csv',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'max_iter',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [200, 500]},\n",
       "    {'parameterName': 'alpha',\n",
       "     'minValue': 2,\n",
       "     'maxValue': 4,\n",
       "     'type': 'DOUBLE',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'}],\n",
       "   'maxTrials': 4,\n",
       "   'maxParallelTrials': 4,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://hostedkfp-default-t0ns1asff6/jobs/JOB_20200821_003637',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/dmgcp-pkg-internal-poc-oct-04/trainer_image:latest'}},\n",
       " 'createTime': '2020-08-21T00:36:39Z',\n",
       " 'startTime': '2020-08-21T00:36:41Z',\n",
       " 'endTime': '2020-08-21T00:46:45Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'completedTrialCount': '4',\n",
       "  'trials': [{'trialId': '2',\n",
       "    'hyperparameters': {'max_iter': '200', 'alpha': '3.4382479813356506'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9993188486796144},\n",
       "    'startTime': '2020-08-21T00:37:18.016709243Z',\n",
       "    'endTime': '2020-08-21T00:45:51Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '1',\n",
       "    'hyperparameters': {'alpha': '3', 'max_iter': '500'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9991616599133715},\n",
       "    'startTime': '2020-08-21T00:37:18.016528543Z',\n",
       "    'endTime': '2020-08-21T00:45:38Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '4',\n",
       "    'hyperparameters': {'alpha': '2.4557425053858326', 'max_iter': '200'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9991267290764286},\n",
       "    'startTime': '2020-08-21T00:37:18.016839932Z',\n",
       "    'endTime': '2020-08-21T00:45:54Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '3',\n",
       "    'hyperparameters': {'alpha': '2.9637090583371171', 'max_iter': '200'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9989520748917144},\n",
       "    'startTime': '2020-08-21T00:37:18.016781136Z',\n",
       "    'endTime': '2020-08-21T00:45:52Z',\n",
       "    'state': 'SUCCEEDED'}],\n",
       "  'consumedMLUnits': 0.26,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': 'Q4Jug/vQV5o='}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned run results are sorted by a value of the optimization metric. The best run is the first item on the returned list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trialId': '2',\n",
       " 'hyperparameters': {'max_iter': '200', 'alpha': '3.4382479813356506'},\n",
       " 'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9993188486796144},\n",
       " 'startTime': '2020-08-21T00:37:18.016709243Z',\n",
       " 'endTime': '2020-08-21T00:45:51Z',\n",
       " 'state': 'SUCCEEDED'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model with the best hyperparameters\n",
    "\n",
    "You can now retrain the model using the best hyperparameters and using combined training and validation splits as a training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = response['trainingOutput']['trials'][0]['hyperparameters']['alpha']\n",
    "max_iter = response['trainingOutput']['trials'][0]['hyperparameters']['max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path=$VALIDATION_FILE_PATH \\\n",
    "--alpha=$alpha \\\n",
    "--max_iter=$max_iter \\\n",
    "--nohptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the training output\n",
    "\n",
    "The training script saved the trained model as the 'model.pkl' in the `JOB_DIR` folder on GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to AI Platform Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'forest_cover_classifier'\n",
    "labels = \"task=classifier,domain=forestry\"\n",
    "filter = 'name:{}'.format(model_name)\n",
    "models = !(gcloud ai-platform models list --filter={filter} --format='value(name)')\n",
    "\n",
    "if not models:\n",
    "    !gcloud ai-platform models create  $model_name \\\n",
    "    --regions=$REGION \\\n",
    "    --labels=$labels\n",
    "else:\n",
    "    print(\"Model: {} already exists.\".format(models[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 'v01'\n",
    "filter = 'name:{}'.format(model_version)\n",
    "versions = !(gcloud ai-platform versions list --model={model_name} --format='value(name)' --filter={filter})\n",
    "\n",
    "if not versions:\n",
    "    !gcloud ai-platform versions create {model_version} \\\n",
    "    --model={model_name} \\\n",
    "    --origin=$JOB_DIR \\\n",
    "    --runtime-version=1.15 \\\n",
    "    --framework=scikit-learn \\\n",
    "    --python-version=3.7\n",
    "else:\n",
    "    print(\"Model version: {} already exists.\".format(versions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve predictions\n",
    "#### Prepare the input file with JSON formated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'serving_instances.json'\n",
    "\n",
    "with open(input_file, 'w') as f:\n",
    "    for index, row in X_validation.head().iterrows():\n",
    "        f.write(json.dumps(list(row.values)))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "--model $model_name \\\n",
    "--version $model_version \\\n",
    "--json-instances $input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
